{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "unseen = pd.read_csv(\"../data/unseen.csv\")\n",
    "seen = pd.concat(\n",
    "    [pd.read_csv(\"../data/train.csv\"), pd.read_csv(\"../data/validation.csv\")]\n",
    ")[[\"comment_id\", \"text_original\"]]\n",
    "unseen = unseen[[\"comment_id\", \"text_original\"]]\n",
    "df = pd.concat([unseen, seen])\n",
    "df = df.drop_duplicates(subset=[\"comment_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "KZ_LETTERS = set(\"әғқңөұүіӘҒҚҢӨҰҮІ\")\n",
    "\n",
    "\n",
    "def is_kazakh_heuristic(text: str, threshold: float = 0.02) -> bool:\n",
    "    \"\"\"\n",
    "    threshold = доля 'казахских' букв среди всех букв.\n",
    "    Если >= threshold — считаем текст казахским.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    letters = [ch for ch in text if ch.isalpha()]\n",
    "    if not letters:\n",
    "        return False\n",
    "\n",
    "    kz_count = sum(1 for ch in letters if ch in KZ_LETTERS)\n",
    "    ratio = kz_count / len(letters)\n",
    "\n",
    "    return ratio >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e723057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_kazakh_heuristic\"] = df[\"text_original\"].apply(is_kazakh_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07978172",
   "metadata": {},
   "outputs": [],
   "source": [
    "KZ_TO_RU_MAP = str.maketrans(\n",
    "    {\n",
    "        \"ә\": \"а\",\n",
    "        \"ғ\": \"г\",\n",
    "        \"қ\": \"к\",\n",
    "        \"ң\": \"н\",\n",
    "        \"ө\": \"о\",\n",
    "        \"ұ\": \"у\",\n",
    "        \"ү\": \"у\",\n",
    "        \"і\": \"и\",\n",
    "        \"Ә\": \"А\",\n",
    "        \"Ғ\": \"Г\",\n",
    "        \"Қ\": \"К\",\n",
    "        \"Ң\": \"Н\",\n",
    "        \"Ө\": \"О\",\n",
    "        \"Ұ\": \"У\",\n",
    "        \"Ү\": \"У\",\n",
    "        \"І\": \"И\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def kz_to_ru_approx(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return text.translate(KZ_TO_RU_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27b6b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Отбираем \"казахские\" и \"русские\" тексты\n",
    "df_kz = df[df[\"is_kazakh_heuristic\"]].copy()\n",
    "df_ru = df[~df[\"is_kazakh_heuristic\"]].copy()\n",
    "\n",
    "# Можно чуть подсэмплить, чтобы классы были сопоставимы по размеру\n",
    "# например, ограничить до N примеров на класс\n",
    "N_KZ = min(len(df_kz), 50_000)\n",
    "N_RU = min(len(df_ru), 50_000)\n",
    "\n",
    "df_kz = df_kz.sample(N_KZ, random_state=42)\n",
    "df_ru = df_ru.sample(N_RU, random_state=42)\n",
    "\n",
    "df_kz[\"lang\"] = \"kk\"\n",
    "df_ru[\"lang\"] = \"ru\"\n",
    "\n",
    "df_lang = pd.concat([df_kz, df_ru], ignore_index=True)\n",
    "df_lang = df_lang.dropna(subset=[\"text_original\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c37ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём дополнительную копию казахских текстов без спецбукв\n",
    "df_kz_aug = df_kz.copy()\n",
    "df_kz_aug[\"text_original\"] = df_kz_aug[\"text_original\"].apply(kz_to_ru_approx)\n",
    "# тот же label\n",
    "df_lang = pd.concat([df_lang, df_kz_aug], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde432b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          kk       0.96      0.94      0.95     11220\n",
      "          ru       0.94      0.95      0.95      9999\n",
      "\n",
      "    accuracy                           0.95     21219\n",
      "   macro avg       0.95      0.95      0.95     21219\n",
      "weighted avg       0.95      0.95      0.95     21219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = df_lang[\"text_original\"].astype(str)\n",
    "y = df_lang[\"lang\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pipeline_lang = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                analyzer=\"char\",\n",
    "                ngram_range=(3, 5),\n",
    "                min_df=5,  # можно тюнить\n",
    "                max_features=200_000,  # можно уменьшить, если мало памяти\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, n_jobs=-1, class_weight=\"balanced\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_lang.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, pipeline_lang.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c45dc41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/langid_ru_kk_model.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(pipeline_lang, \"model/langid_ru_kk_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "pipeline_lang = joblib.load(\"model/langid_ru_kk_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1afaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ru', 'ru'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_lang.predict([\"ф\", \"фыа\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2eacf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
